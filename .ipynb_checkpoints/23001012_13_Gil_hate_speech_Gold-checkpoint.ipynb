{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918862e-c6d5-40d4-981a-90cba97a083d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cd429-ea75-4130-92f4-daf7980ba346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding= 'latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d887812-a191-46af-ad90-f667db1b8852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67172066-67e8-4b0f-874b-73e0041777d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aed2b2-1a11-4a90-8647-5e1b86342d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abusive_dict = pd.read_csv('abusive.csv', encoding='latin-1')\n",
    "abusive_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064019c-4818-4fcb-9ca6-66b2f72ebc86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "             \n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('rt',' ',text)\n",
    "    text = re.sub('RT',' ',text)\n",
    "    text = re.sub('user',' ',text)\n",
    "    text = re.sub('USER',' ',text)\n",
    "    text = re.sub('((www\\,[^\\s]+)|(http?://[^\\s)+))',' ',text)\n",
    "    text = re.sub(' +',' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]',' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_abusive(text):\n",
    "    text = ' '.join(['' if word in abusive_dict.ABUSIVE.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "print(\"lowercase: \", lowercase(\"Haloooo, duniaa!\"))\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,,dunia!!\"))\n",
    "\n",
    "print(\"remove_abusive: \", remove_abusive(\"anak anjing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186e3fe-c0a2-43ec-8486-0c6ece5e7fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_abusive(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e00d88-5b1c-459d-97da-f8c8080fe3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95f478-7a58-4cd1-a63d-434a46a86a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e84a4-741e-4ae4-bb7d-c28ba90c679c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder\n",
    "from flasgger import swag_from\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Data Processing and Modeling'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Data Processing dan Modeling'),\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json',\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,             \n",
    "                  config=swagger_config)\n",
    "\n",
    "\n",
    "df_abusive = pd.read_csv('abusive.csv')\n",
    "df_kamusalay = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "df_kamusalay.columns=[\"tidak baku\",\"baku\"]\n",
    "\n",
    "\n",
    "@swag_from(\"C:/Users/skyne/Documents/docs/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Menyapa Hello World\",\n",
    "        'data': \"Hello World\",\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "@swag_from(\"C:/Users/skyne/Documents/docs/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "def text_processing():\n",
    "    global text,new_list\n",
    "    text = request.form.get('text')\n",
    "    \n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('rt',' ',text)\n",
    "    text = re.sub('RT',' ',text)\n",
    "    text = re.sub('user',' ',text)\n",
    "    text = re.sub('USER',' ',text)\n",
    "    text = re.sub('((www\\,[^\\s]+)|(http?://[^\\s)+))',' ',text)\n",
    "    text = re.sub(' +',' ',text)\n",
    "    return text\n",
    "\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': re.sub(r'[^a-zA-Z0-9]',' ', text)\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "                                                    \n",
    "@swag_from(\"C:/Users/skyne/Documents/docs/file_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    global post_df\n",
    "    \n",
    "    file = request.files.get('file')\n",
    "    post_df = pd.read_csv(file, encoding='latin-1')\n",
    "    post_df = post_df[['Tweet']]\n",
    "    post_df.drop_duplicates(inplace=True)\n",
    "    post_df['no_char'] = post_df['Tweet'].apply(len)\n",
    "    post_df['no_words'] = post_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    def tweet_cleansing(x):\n",
    "        tweet = x\n",
    "        cleaned_tweet = re.sub(r'[^a-zA-Z0-9 ]','',tweet).strip()\n",
    "        return cleaned_tweet\n",
    "    \n",
    "    post_df['cleaned_tweet'] = post_df['Tweet'].apply(lambda x: tweet_cleansing(x))\n",
    "    post_df['no_char_2'] = post_df['cleaned_tweet'].apply(len)\n",
    "    post_df['no_words_2'] = post_df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    def count_abusive(x):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "    \n",
    "    post_df['estimated_no_abs_words'] = post_df['cleaned_tweet'].apply(lambda x: count_abusive(x))\n",
    "    \n",
    "    conn = sqlite3.connect('database_project.db')\n",
    "    q_create_table = \"\"\"\n",
    "    create table post_df (Tweet varchar(255), no_char int, no_words int, cleaned_tweet varchar(255), no_char_2 int, no_words_2 int);\n",
    "    \"\"\"\n",
    "    conn.execute(q_create_table)\n",
    "    conn.commit()\n",
    "    \n",
    "    for i in range(len(post_df)):\n",
    "        tweet = post_df['Tweet'].iloc[i]\n",
    "        no_char = int(post_df['no_char'].iloc[i])\n",
    "        no_words = int(post_df['no_words'].iloc[i])\n",
    "        cleaned_tweet = post_df['cleaned_tweet'].iloc[i]\n",
    "        no_char_2 = int(post_df['no_char_2'].iloc[i])\n",
    "        no_words_2 = int(post_df['no_words_2'].iloc[i])\n",
    "    \n",
    "        q_insertion = \"insert into post_df (Tweet, no_char, no_words, cleaned_tweet, no_char_2, no_words_2) values (?,?,?,?,?,?)\"\n",
    "        conn.execute(q_insertion,(tweet,no_char,no_words,cleaned_tweet,no_char_2,no_words_2))\n",
    "        conn.commit()\n",
    "        \n",
    "    conn.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "    countplot = sns.countplot(data=post_df, x=\"estimated_no_abs_words\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center'\n",
    "                            , va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=post_df, x=\"no_words_2\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('new_boxplot.jpeg')\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': list(post_df['cleaned_tweet'])\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e7119-f180-4c5d-9b93-8fb935338b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_3",
   "language": "python",
   "name": "myenv_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
